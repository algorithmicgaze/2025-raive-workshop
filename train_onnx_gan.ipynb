{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_mp0c0k8rCS"
   },
   "source": [
    "# Training a PIX2PIX Model using PyTorch / ONNX\n",
    "\n",
    "This notebook walks you through the steps of training your own image-to-image machine learning model.\n",
    "\n",
    "Basically all you have to do is put your cursor in a cell and press Shift+Enter. At the end, you can download the latest model from the `output` folder (it will be called something like `generator_epoch_XXX.onnx`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqhpNeaL8z7C",
    "outputId": "bd15bb05-4640-4dd2-c43c-08fd8f0c70e7"
   },
   "outputs": [],
   "source": [
    "# Make sure you are connected to a runtime with a GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFa-5ik3_MFq",
    "outputId": "f5eeabe8-246d-49f4-cced-1ad32e0b5b02"
   },
   "outputs": [],
   "source": [
    "# Install ONNX (not installed by default)\n",
    "#import locale\n",
    "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "%pip install -q onnx matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcftxzAmuAzP"
   },
   "outputs": [],
   "source": [
    "# Import all other dependencies\n",
    "import glob, os, random, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJijQKrV9J93",
    "outputId": "fb4fc26a-ce98-4baf-c5c9-397f5ce09d2f"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(\"GPU is\", \"available\" if gpu_available else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the dataset\n",
    "!curl -O https://algorithmicgaze.s3.amazonaws.com/workshops/2025-raive/patterns_512.zip\n",
    "!mkdir -p datasets/patterns\n",
    "!unzip -j -o -qq *.zip -d datasets/patterns\n",
    "!rm -r datasets/patterns/._*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-1v5IkJ-Nex"
   },
   "outputs": [],
   "source": [
    "# Some helper functions for creating/checking directories.\n",
    "def directory_should_exist(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    if not os.path.isdir(dir):\n",
    "        raise Exception(\"Path '{}' is not a directory.\".format(dir))\n",
    "    return dir\n",
    "\n",
    "def ensure_directory(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2O3py7u-JC"
   },
   "outputs": [],
   "source": [
    "# Point to your dataset and configure training\n",
    "input_dir = directory_should_exist(\"datasets/patterns\")\n",
    "output_dir = ensure_directory(\"output\")\n",
    "\n",
    "# I/O and schedule\n",
    "epochs = 100\n",
    "batch_size = 8              # 512x512 is heavy; tweak based on VRAM\n",
    "sample_interval = 50        # iterations between samples\n",
    "snapshot_interval = 1       # epochs between checkpoints\n",
    "\n",
    "# Latent & image\n",
    "z_dim = 128\n",
    "img_channels = 3\n",
    "image_size = 512\n",
    "\n",
    "# Optim & regularization (modern defaults for SN+hinge)\n",
    "g_lr = 2e-4\n",
    "d_lr = 2e-4\n",
    "betas = (0.0, 0.99)\n",
    "\n",
    "# Regularization / stability\n",
    "d_reg_every = 16           # R1 every N D steps (lazy)\n",
    "r1_gamma = 10.0            # R1 weight (StyleGAN2 uses 10 at 256; 10 is fine here)\n",
    "ema_decay = 0.999          # EMA for generator weights\n",
    "use_amp = True             # mixed precision for speed\n",
    "torch.backends.cudnn.benchmark = True  # speed-up on constant shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJuVKp5GujGO"
   },
   "outputs": [],
   "source": [
    "# Unconditional image dataset (no splitting)\n",
    "class UncondImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(root_dir)\n",
    "                            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTRhTxTUNd-d"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # -> [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = UncondImageDataset(input_dir, transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "z1CkVmZINZo8",
    "outputId": "5856e7ec-827d-499a-bcfa-5c0e2f16c880"
   },
   "outputs": [],
   "source": [
    "# Show a single real image from the dataset\n",
    "def plot_image(ax, title, img):\n",
    "    img = (img + 1) / 2\n",
    "    ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    ax.set_title(title); ax.axis(\"off\")\n",
    "\n",
    "assert len(dataset) > 0, f\"No images found in {input_dir}. Supported: .jpg .jpeg .png\"\n",
    "real_batch = next(iter(dataloader))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "plot_image(ax, \"Real Image\", real_batch[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0S8t5BySulh3"
   },
   "outputs": [],
   "source": [
    "# Generator building block: nearest upsample + 3x3 conv + BN + LeakyReLU\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFmjby3Tum_G"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Fast-ish DCGAN-style generator for 512x512.\n",
    "    Channel schedule favors speed; adjust if you need more fidelity.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=128, img_channels=3, chs=(512, 512, 256, 128, 64, 32, 16)):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.chs = chs\n",
    "        # Map z to 4x4xchs[0]\n",
    "        self.fc = nn.Linear(z_dim, 4 * 4 * chs[0])\n",
    "        # Upsampling stack: 4→8→16→...→512\n",
    "        blocks = []\n",
    "        for cin, cout in zip(chs[:-1], chs[1:]):\n",
    "            blocks.append(UpBlock(cin, cout))\n",
    "        self.up = nn.Sequential(*blocks)\n",
    "        self.to_rgb = nn.Conv2d(chs[-1], img_channels, kernel_size=1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, z):\n",
    "        if z.dim() == 2:\n",
    "            x = self.fc(z).view(-1, self.chs[0], 4, 4)\n",
    "        else:\n",
    "            # allow [B, z, 1, 1]\n",
    "            x = self.fc(z.view(z.size(0), -1)).view(-1, self.chs[0], 4, 4)\n",
    "        x = self.up(x)\n",
    "        x = torch.tanh(self.to_rgb(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XoK7HKXuofx"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    SN discriminator for 512x512 using strided 4x4 convs.\n",
    "    Uses global average pooling before the final linear head\n",
    "    so it adapts to whatever final channel count the body yields.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_channels=3, chs=(64, 128, 256, 512, 512, 512, 512)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_ch = img_channels\n",
    "        for out_ch in chs:\n",
    "            layers += [\n",
    "                spectral_norm(nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1)),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.body = nn.Sequential(*layers)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)         # -> [B, C, 1, 1]\n",
    "        self.head = spectral_norm(nn.Linear(in_ch, 1))  # maps [B, C] -> [B, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.body(x)                 # [B, C, H, W]\n",
    "        h = self.pool(h).view(h.size(0), -1)  # [B, C]\n",
    "        logits = self.head(h).squeeze(1) # [B]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuhzmZFquqEg"
   },
   "outputs": [],
   "source": [
    "# --- Training helpers: R1, EMA, Hinge losses ---\n",
    "\n",
    "def r1_penalty(d_out, real_img):\n",
    "    # ∇_x D(x) on real images\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_out.sum(), inputs=real_img,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    return grads.pow(2).reshape(grads.size(0), -1).sum(dim=1).mean()\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = copy.deepcopy(model).eval()\n",
    "        for p in self.shadow.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        msd = self.shadow.state_dict()\n",
    "        for k, src in model.state_dict().items():\n",
    "            tgt = msd[k]\n",
    "            # Only blend floating-point tensors; copy others (e.g., integer buffers)\n",
    "            if torch.is_floating_point(tgt):\n",
    "                # tgt = decay * tgt + (1 - decay) * src\n",
    "                tgt.mul_(self.decay).add_(src, alpha=1.0 - self.decay)\n",
    "            else:\n",
    "                tgt.copy_(src)\n",
    "\n",
    "# Hinge loss:\n",
    "#   D:  E[max(0, 1 - D(real))] + E[max(0, 1 + D(fake))]\n",
    "#   G: -E[D(fake)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "968aaAGLurrj"
   },
   "outputs": [],
   "source": [
    "# Load snapshot if available\n",
    "def get_latest_snapshot(output_dir):\n",
    "    snapshots = glob.glob(os.path.join(output_dir, \"snapshot_epoch_*.pth\"))\n",
    "    if not snapshots:\n",
    "        return None\n",
    "    return max(snapshots, key=os.path.getctime)\n",
    "\n",
    "def get_latest_generator(output_dir):\n",
    "    generators = glob.glob(os.path.join(output_dir, \"generator_epoch_*.onnx\"))\n",
    "    if not generators:\n",
    "        return None\n",
    "    return max(generators, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqrPl_uVuuEc"
   },
   "outputs": [],
   "source": [
    "def load_snapshot(generator, discriminator, g_optimizer, d_optimizer, snapshot_path, ema_shadow=None):\n",
    "    checkpoint = torch.load(snapshot_path, map_location=device, weights_only=False)\n",
    "    generator.load_state_dict(checkpoint[\"generator\"])\n",
    "    discriminator.load_state_dict(checkpoint[\"discriminator\"])\n",
    "    g_optimizer.load_state_dict(checkpoint[\"g_optimizer\"])\n",
    "    d_optimizer.load_state_dict(checkpoint[\"d_optimizer\"])\n",
    "    if ema_shadow is not None and \"ema_generator\" in checkpoint:\n",
    "        ema_shadow.load_state_dict(checkpoint[\"ema_generator\"])\n",
    "    start_epoch = int(os.path.basename(snapshot_path).split(\"_\")[2].split(\".\")[0])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO3efseEuvlZ"
   },
   "outputs": [],
   "source": [
    "# Create the training loop (unconditional GAN, hinge + R1, SN-D, EMA, AMP)\n",
    "def train(generator, discriminator, dataloader, opts):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.train().to(device)\n",
    "    discriminator.train().to(device)\n",
    "\n",
    "    g_opt = optim.Adam(generator.parameters(), lr=opts.g_lr, betas=opts.betas)\n",
    "    d_opt = optim.Adam(discriminator.parameters(), lr=opts.d_lr, betas=opts.betas)\n",
    "\n",
    "    scaler_g = GradScaler(enabled=opts.use_amp)\n",
    "    scaler_d = GradScaler(enabled=opts.use_amp)\n",
    "\n",
    "    ema = EMA(generator, decay=opts.ema_decay)\n",
    "\n",
    "    # Fixed latent for monitoring\n",
    "    fixed_z = torch.randn(16, opts.z_dim, device=device)\n",
    "\n",
    "    start_epoch = 1\n",
    "    if not getattr(opts, \"restart\", False):\n",
    "        latest_snapshot = get_latest_snapshot(opts.output_dir)\n",
    "        if latest_snapshot:\n",
    "            start_epoch = load_snapshot(\n",
    "                generator, discriminator, g_opt, d_opt, latest_snapshot, ema_shadow=ema.shadow\n",
    "            )\n",
    "            print(f\"Resumed from {latest_snapshot} (start_epoch={start_epoch})\")\n",
    "\n",
    "    it = 0\n",
    "    for epoch in range(start_epoch, opts.epochs + 1):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{opts.epochs}\")\n",
    "        for real in pbar:\n",
    "            it += 1\n",
    "            real = real.to(device, non_blocking=True)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator\n",
    "            # -----------------------\n",
    "            z = torch.randn(bsz, opts.z_dim, device=device)\n",
    "            with autocast(enabled=opts.use_amp):\n",
    "                with torch.no_grad():\n",
    "                    fake = generator(z)\n",
    "                d_real = discriminator(real)\n",
    "                d_fake = discriminator(fake)\n",
    "                d_loss = F.relu(1.0 - d_real).mean() + F.relu(1.0 + d_fake).mean()\n",
    "\n",
    "            d_opt.zero_grad(set_to_none=True)\n",
    "            scaler_d.scale(d_loss).backward()\n",
    "            scaler_d.step(d_opt)\n",
    "            scaler_d.update()\n",
    "\n",
    "            # Lazy R1 regularization on real images\n",
    "            if (it % opts.d_reg_every) == 0:\n",
    "                real_req = real.detach().requires_grad_(True)\n",
    "                # Compute R1 in FP32 for stability\n",
    "                with autocast(enabled=False):\n",
    "                    d_real_r1 = discriminator(real_req.float())\n",
    "                    r1 = r1_penalty(d_real_r1, real_req.float())\n",
    "                    r1_loss = (opts.r1_gamma / 2.0) * r1\n",
    "                d_opt.zero_grad(set_to_none=True)\n",
    "                scaler_d.scale(r1_loss).backward()\n",
    "                scaler_d.step(d_opt)\n",
    "                scaler_d.update()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Generator\n",
    "            # -----------------------\n",
    "            z = torch.randn(bsz, opts.z_dim, device=device)\n",
    "            with autocast(enabled=opts.use_amp):\n",
    "                fake = generator(z)\n",
    "                g_fake = discriminator(fake)\n",
    "                g_loss = -g_fake.mean()\n",
    "\n",
    "            g_opt.zero_grad(set_to_none=True)\n",
    "            scaler_g.scale(g_loss).backward()\n",
    "            scaler_g.step(g_opt)\n",
    "            scaler_g.update()\n",
    "\n",
    "            # EMA update\n",
    "            ema.update(generator)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"D\": f\"{d_loss.item():.3f}\",\n",
    "                \"G\": f\"{g_loss.item():.3f}\",\n",
    "            })\n",
    "\n",
    "            # Visualization and sampling\n",
    "            if it % opts.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    generator.eval()\n",
    "                    samples = ema.shadow(fixed_z).detach().cpu()\n",
    "                    # Save and show\n",
    "                    save_path = os.path.join(opts.output_dir, f\"epoch_{epoch}_iter_{it}.jpg\")\n",
    "                    save_image(samples, save_path, nrow=4, normalize=True, value_range=(-1, 1))\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"Epoch {epoch} | iter {it}\")\n",
    "                    grid = (samples[:4] + 1) / 2\n",
    "                    fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
    "                    for a, img in zip(axes, grid):\n",
    "                        a.imshow(img.permute(1, 2, 0).numpy()); a.axis(\"off\")\n",
    "                    plt.show()\n",
    "                    generator.train()\n",
    "\n",
    "        # Snapshot & ONNX export at epoch end\n",
    "        if (epoch % opts.snapshot_interval) == 0:\n",
    "            snap_path = os.path.join(opts.output_dir, f\"snapshot_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                \"generator\": generator.state_dict(),\n",
    "                \"discriminator\": discriminator.state_dict(),\n",
    "                \"g_optimizer\": g_opt.state_dict(),\n",
    "                \"d_optimizer\": d_opt.state_dict(),\n",
    "                \"ema_generator\": ema.shadow.state_dict(),\n",
    "            }, snap_path)\n",
    "            print(f\"Saved snapshot to {snap_path}\")\n",
    "\n",
    "            # Export ONNX from EMA generator (better inference)\n",
    "            ema.shadow.eval()\n",
    "            dummy_z = torch.randn(1, opts.z_dim, device=device)\n",
    "            traced = torch.jit.trace(ema.shadow, dummy_z)\n",
    "            onnx_path = os.path.join(opts.output_dir, f\"generator_epoch_{epoch}.onnx\")\n",
    "            torch.onnx.export(\n",
    "                traced,\n",
    "                dummy_z,\n",
    "                onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=[\"z\"],\n",
    "                output_names=[\"image\"],\n",
    "                dynamic_axes={\"z\": {0: \"batch\"}, \"image\": {0: \"batch\"}},\n",
    "            )\n",
    "            print(f\"ONNX model exported to {onnx_path}\")\n",
    "            ema.shadow.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "2QUHdooiuztD",
    "outputId": "a26f1c1a-794b-4cfc-9ef8-0dff8dbd83a0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(z_dim=z_dim, img_channels=img_channels).to(device)\n",
    "discriminator = Discriminator(img_channels=img_channels).to(device)\n",
    "\n",
    "opts = SimpleNamespace(\n",
    "    output_dir=output_dir,\n",
    "    sample_interval=sample_interval,\n",
    "    snapshot_interval=snapshot_interval,\n",
    "    epochs=epochs,\n",
    "    restart=False,\n",
    "    # new:\n",
    "    z_dim=z_dim,\n",
    "    g_lr=g_lr, d_lr=d_lr, betas=betas,\n",
    "    d_reg_every=d_reg_every, r1_gamma=r1_gamma,\n",
    "    ema_decay=ema_decay, use_amp=use_amp,\n",
    ")\n",
    "\n",
    "train(generator, discriminator, dataloader, opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4IWPRe7YESG"
   },
   "source": [
    "## Optional: Copy the generator model to Google Drive\n",
    "\n",
    "You can save some time downloading/uploading the generator by using Google Drive. In the next steps we'll connect to Google Drive and upload the generator. The conversion Colab notebook will look for this zip file and convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6kTEBQCYHzu",
    "outputId": "93dcf344-754f-4ef6-bf38-89d8632333f8"
   },
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive. This will ask for permissions.\n",
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-K_jwLSyTiS2",
    "outputId": "cc7d5f07-4b88-4eb4-ecbb-aba80ae4df11"
   },
   "outputs": [],
   "source": [
    "# Step 2: Copy the generator.zip to Google Drive\n",
    "import shutil\n",
    "drive_folder = '/drive/MyDrive/2025-raive'\n",
    "ensure_directory(drive_folder)\n",
    "shutil.copy(get_latest_generator(output_dir), drive_folder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
