{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_mp0c0k8rCS"
   },
   "source": [
    "# Training a PIX2PIX Model using PyTorch / ONNX\n",
    "\n",
    "This notebook walks you through the steps of training your own image-to-image machine learning model.\n",
    "\n",
    "Basically all you have to do is put your cursor in a cell and press Shift+Enter. At the end, you can download the latest model from the `output` folder (it will be called something like `generator_epoch_XXX.onnx`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqhpNeaL8z7C",
    "outputId": "bd15bb05-4640-4dd2-c43c-08fd8f0c70e7"
   },
   "outputs": [],
   "source": [
    "# Make sure you are connected to a runtime with a GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading your images\n",
    "Once the cell above has executed, you're connected to a runtime (a machine in the cloud). Click the folder icon in the left sidebar, then drag your ZIP file with files on the sidebar to upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFa-5ik3_MFq",
    "outputId": "f5eeabe8-246d-49f4-cced-1ad32e0b5b02"
   },
   "outputs": [],
   "source": [
    "# Install ONNX (not installed by default)\n",
    "#import locale\n",
    "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "%pip install -q onnx tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcftxzAmuAzP"
   },
   "outputs": [],
   "source": [
    "# Import all other dependencies\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJijQKrV9J93",
    "outputId": "fb4fc26a-ce98-4baf-c5c9-397f5ce09d2f"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(\"GPU is\", \"available\" if gpu_available else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tO0EYyUMkCe",
    "outputId": "478c04ac-4edf-41fc-c6fc-0a5964a9a399"
   },
   "outputs": [],
   "source": [
    "# Download and unzip the dataset\n",
    "#!curl -O https://algorithmicgaze.s3.amazonaws.com/workshops/2025-raive/patterns_512.zip\n",
    "#!mkdir -p datasets/patterns\n",
    "#!unzip -j -o -qq *.zip -d datasets/patterns\n",
    "#!rm -r datasets/patterns/._*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-1v5IkJ-Nex"
   },
   "outputs": [],
   "source": [
    "# Some helper functions for creating/checking directories.\n",
    "def directory_should_exist(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    if not os.path.isdir(dir):\n",
    "        raise Exception(\"Path '{}' is not a directory.\".format(dir))\n",
    "    return dir\n",
    "\n",
    "def ensure_directory(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2O3py7u-JC"
   },
   "outputs": [],
   "source": [
    "input_dir = directory_should_exist(\"datasets/patterns\")\n",
    "output_dir = ensure_directory(\"output\")\n",
    "sample_interval = 300\n",
    "snapshot_interval = 1\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJuVKp5GujGO"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [\n",
    "            f for f in os.listdir(root_dir) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTRhTxTUNd-d"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(input_dir, transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "z1CkVmZINZo8",
    "outputId": "5856e7ec-827d-499a-bcfa-5c0e2f16c880"
   },
   "outputs": [],
   "source": [
    "# Show a single image from the dataset\n",
    "def plot_image(img):\n",
    "    img = (img + 1) / 2 # Denormalize\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "sample_img = next(iter(dataloader))\n",
    "plot_image(sample_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFmjby3Tum_G"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: z_dim x 1 x 1\n",
    "            nn.ConvTranspose2d(z_dim, 1024, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            # State: 1024 x 4 x 4\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # State: 512 x 8 x 8\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # State: 256 x 16 x 16\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # State: 128 x 32 x 32\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # State: 64 x 64 x 64\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            # State: 32 x 128 x 128\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            # State: 16 x 256 x 256\n",
    "            nn.ConvTranspose2d(16, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output: 3 x 512 x 512\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XoK7HKXuofx"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: 3 x 512 x 512\n",
    "            spectral_norm(nn.Conv2d(3, 16, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: 16 x 256 x 256\n",
    "            spectral_norm(nn.Conv2d(16, 32, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: 32 x 128 x 128\n",
    "            spectral_norm(nn.Conv2d(32, 64, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: 64 x 64 x 64\n",
    "            spectral_norm(nn.Conv2d(64, 128, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: 128 x 32 x 32\n",
    "            spectral_norm(nn.Conv2d(128, 256, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: 256 x 16 x 16\n",
    "            spectral_norm(nn.Conv2d(256, 512, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: 512 x 8 x 8\n",
    "            spectral_norm(nn.Conv2d(512, 1024, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: 1024 x 4 x 4\n",
    "            spectral_norm(nn.Conv2d(1024, 1, 4, 1, 0, bias=False)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuhzmZFquqEg"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "968aaAGLurrj"
   },
   "outputs": [],
   "source": [
    "# Load snapshot if available\n",
    "def get_latest_snapshot(output_dir):\n",
    "    snapshots = glob.glob(os.path.join(output_dir, \"snapshot_epoch_*.pth\"))\n",
    "    if not snapshots:\n",
    "        return None\n",
    "    return max(snapshots, key=os.path.getctime)\n",
    "\n",
    "def get_latest_generator(output_dir):\n",
    "    generators = glob.glob(os.path.join(output_dir, \"generator_epoch_*.onnx\"))\n",
    "    if not generators:\n",
    "        return None\n",
    "    return max(generators, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqrPl_uVuuEc"
   },
   "outputs": [],
   "source": [
    "def load_snapshot(generator, discriminator, g_optimizer, d_optimizer, snapshot_path):\n",
    "    checkpoint = torch.load(snapshot_path, map_location=device, weights_only=False)\n",
    "    generator.load_state_dict(checkpoint[\"generator\"])\n",
    "    discriminator.load_state_dict(checkpoint[\"discriminator\"])\n",
    "    g_optimizer.load_state_dict(checkpoint[\"g_optimizer\"])\n",
    "    d_optimizer.load_state_dict(checkpoint[\"d_optimizer\"])\n",
    "    start_epoch = int(os.path.basename(snapshot_path).split(\"_\")[2].split(\".\")[0])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO3efseEuvlZ"
   },
   "outputs": [],
   "source": [
    "def train(generator, discriminator, dataloader, opts):\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "    # Create a fixed noise vector for visualization\n",
    "    fixed_noise = torch.randn(16, opts.latent_dim, 1, 1, device=device)\n",
    "\n",
    "    start_epoch = 1\n",
    "    if not opts.restart:\n",
    "        latest_snapshot = get_latest_snapshot(opts.output_dir)\n",
    "        if latest_snapshot:\n",
    "            start_epoch = load_snapshot(\n",
    "                generator, discriminator, g_optimizer, d_optimizer, latest_snapshot\n",
    "            ) + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(\"No snapshots found. Starting from scratch.\")\n",
    "    else:\n",
    "        print(\"Restarting training from scratch.\")\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + opts.epochs):\n",
    "        for i, real_images in enumerate(tqdm(dataloader)):\n",
    "            real_images = real_images.to(device)\n",
    "            b_size = real_images.size(0)\n",
    "\n",
    "            # Create labels for real and fake images\n",
    "            real_label = torch.full((b_size,), 0.9, dtype=torch.float, device=device)\n",
    "            fake_label = torch.full((b_size,), 0.0, dtype=torch.float, device=device)\n",
    "\n",
    "            # --- Train Discriminator ---\n",
    "            d_optimizer.zero_grad()\n",
    "            # Real images\n",
    "            output_real = discriminator(real_images).view(-1)\n",
    "            d_loss_real = criterion(output_real, real_label)\n",
    "            d_loss_real.backward()\n",
    "\n",
    "            # Fake images\n",
    "            noise = torch.randn(b_size, opts.latent_dim, 1, 1, device=device)\n",
    "            fake_images = generator(noise)\n",
    "            output_fake = discriminator(fake_images.detach()).view(-1)\n",
    "            d_loss_fake = criterion(output_fake, fake_label)\n",
    "            d_loss_fake.backward()\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # --- Train Generator ---\n",
    "            g_optimizer.zero_grad()\n",
    "            output_g = discriminator(fake_images).view(-1)\n",
    "            g_loss = criterion(output_g, real_label) # Generator wants discriminator to think fakes are real\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            if i % opts.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    fake_samples = generator(fixed_noise).detach().cpu()\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Epoch [{epoch}/{start_epoch + opts.epochs -1}] Batch {i}/{len(dataloader)} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "                \n",
    "                grid_img = save_image(\n",
    "                    fake_samples,\n",
    "                    f\"{opts.output_dir}/epoch_{epoch}_iter_{i}.jpg\",\n",
    "                    nrow=4,\n",
    "                    normalize=True,\n",
    "                    padding=2\n",
    "                )\n",
    "                \n",
    "                # Create a grid of images\n",
    "                grid = make_grid(\n",
    "                    fake_samples, nrow=4, normalize=True, padding=2\n",
    "                )\n",
    "                \n",
    "                # Display the grid\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(grid.permute(1, 2, 0))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "        if (epoch) % opts.snapshot_interval == 0:\n",
    "            torch.save({\n",
    "                \"generator\": generator.state_dict(),\n",
    "                \"discriminator\": discriminator.state_dict(),\n",
    "                \"g_optimizer\": g_optimizer.state_dict(),\n",
    "                \"d_optimizer\": d_optimizer.state_dict(),\n",
    "            }, f\"{opts.output_dir}/snapshot_epoch_{epoch}.pth\")\n",
    "\n",
    "            # Save to ONNX format\n",
    "            onnx_path = f\"{opts.output_dir}/generator_epoch_{epoch}.onnx\"\n",
    "            generator.eval()\n",
    "            dummy_input = torch.randn(1, opts.latent_dim, 1, 1, device=device)\n",
    "            torch.onnx.export(\n",
    "                generator,\n",
    "                dummy_input,\n",
    "                onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=[\"latent_vector\"],\n",
    "                output_names=[\"output_image\"],\n",
    "                dynamic_axes={\"latent_vector\": {0: \"batch_size\"}, \"output_image\": {0: \"batch_size\"}},\n",
    "            )\n",
    "            print(f\"ONNX model exported to {onnx_path}\")\n",
    "            generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "2QUHdooiuztD",
    "outputId": "a26f1c1a-794b-4cfc-9ef8-0dff8dbd83a0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(z_dim=latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "opts = {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"sample_interval\": sample_interval,\n",
    "    \"snapshot_interval\": snapshot_interval,\n",
    "    \"epochs\": epochs,\n",
    "    \"restart\": False,\n",
    "    \"latent_dim\": latent_dim,\n",
    "}\n",
    "train(generator, discriminator, dataloader, SimpleNamespace(**opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4IWPRe7YESG"
   },
   "source": [
    "## Optional: Copy the generator model to Google Drive\n",
    "\n",
    "Downloading the model from the sidebar can be very slow. You can save some time downloading/uploading the generator by using Google Drive. In the next steps we'll connect to Google Drive and upload the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6kTEBQCYHzu",
    "outputId": "93dcf344-754f-4ef6-bf38-89d8632333f8"
   },
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive. This will ask for permissions.\n",
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-K_jwLSyTiS2",
    "outputId": "cc7d5f07-4b88-4eb4-ecbb-aba80ae4df11"
   },
   "outputs": [],
   "source": [
    "# Step 2: Copy the latest .onnx file to Google Drive\n",
    "import shutil\n",
    "drive_folder = '/drive/MyDrive/2025-raive'\n",
    "ensure_directory(drive_folder)\n",
    "shutil.copy(get_latest_generator(output_dir), drive_folder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
