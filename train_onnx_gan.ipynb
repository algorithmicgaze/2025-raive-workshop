{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_mp0c0k8rCS"
   },
   "source": [
    "# Training a PIX2PIX Model using PyTorch / ONNX\n",
    "\n",
    "This notebook walks you through the steps of training your own image-to-image machine learning model.\n",
    "\n",
    "Basically all you have to do is put your cursor in a cell and press Shift+Enter. At the end, you can download the latest model from the `output` folder (it will be called something like `generator_epoch_XXX.onnx`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqhpNeaL8z7C",
    "outputId": "bd15bb05-4640-4dd2-c43c-08fd8f0c70e7"
   },
   "outputs": [],
   "source": [
    "# Make sure you are connected to a runtime with a GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFa-5ik3_MFq",
    "outputId": "f5eeabe8-246d-49f4-cced-1ad32e0b5b02"
   },
   "outputs": [],
   "source": [
    "# Install ONNX (not installed by default)\n",
    "#import locale\n",
    "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "%pip install -q onnx matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcftxzAmuAzP"
   },
   "outputs": [],
   "source": [
    "# Import all other dependencies\n",
    "import glob, os, random, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJijQKrV9J93",
    "outputId": "fb4fc26a-ce98-4baf-c5c9-397f5ce09d2f"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(\"GPU is\", \"available\" if gpu_available else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the dataset\n",
    "!curl -O https://algorithmicgaze.s3.amazonaws.com/workshops/2025-raive/patterns_512.zip\n",
    "!mkdir -p datasets/patterns\n",
    "!unzip -j -o -qq *.zip -d datasets/patterns\n",
    "!rm -r datasets/patterns/._*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-1v5IkJ-Nex"
   },
   "outputs": [],
   "source": [
    "# Some helper functions for creating/checking directories.\n",
    "def directory_should_exist(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    if not os.path.isdir(dir):\n",
    "        raise Exception(\"Path '{}' is not a directory.\".format(dir))\n",
    "    return dir\n",
    "\n",
    "def ensure_directory(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2O3py7u-JC"
   },
   "outputs": [],
   "source": [
    "# Point to your dataset and configure training\n",
    "input_dir = directory_should_exist(\"datasets/patterns\")\n",
    "output_dir = ensure_directory(\"output\")\n",
    "\n",
    "# I/O and schedule\n",
    "epochs = 100\n",
    "batch_size = 64              # 512x512 is heavy; tweak based on VRAM\n",
    "sample_interval = 15        # iterations between samples\n",
    "snapshot_interval = 1       # epochs between checkpoints\n",
    "\n",
    "# Latent & image\n",
    "z_dim = 128\n",
    "img_channels = 3\n",
    "image_size = 512\n",
    "\n",
    "# Optim & regularization (modern defaults for SN+hinge)\n",
    "g_lr = 2e-4\n",
    "d_lr = 2e-4\n",
    "betas = (0.0, 0.99)\n",
    "\n",
    "# Regularization / stability\n",
    "d_reg_every = 16           # R1 every N D steps (lazy)\n",
    "r1_gamma = 10.0            # R1 weight (StyleGAN2 uses 10 at 256; 10 is fine here)\n",
    "ema_decay = 0.995          # EMA for generator weights\n",
    "use_amp = True             # mixed precision for speed\n",
    "torch.backends.cudnn.benchmark = True  # speed-up on constant shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJuVKp5GujGO"
   },
   "outputs": [],
   "source": [
    "# Unconditional image dataset (no splitting) + shape guard\n",
    "class UncondImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, expected_hw=None):\n",
    "        \"\"\"\n",
    "        expected_hw: tuple (H, W) to enforce final tensor size, e.g. (512, 512).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.expected_hw = expected_hw\n",
    "        self.image_files = [f for f in os.listdir(root_dir)\n",
    "                            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # [C,H,W], float in [-1,1]\n",
    "        if self.expected_hw is not None:\n",
    "            C, H, W = img.shape\n",
    "            expH, expW = self.expected_hw\n",
    "            if (H, W) != (expH, expW) or C != 3:\n",
    "                raise ValueError(f\"Bad tensor shape {img.shape} for {path}; expected (3,{expH},{expW})\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTRhTxTUNd-d"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # -> [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = UncondImageDataset(input_dir, transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "z1CkVmZINZo8",
    "outputId": "5856e7ec-827d-499a-bcfa-5c0e2f16c880"
   },
   "outputs": [],
   "source": [
    "# Show a single real image from the dataset\n",
    "def plot_image(ax, title, img):\n",
    "    img = (img + 1) / 2\n",
    "    ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    ax.set_title(title); ax.axis(\"off\")\n",
    "\n",
    "assert len(dataset) > 0, f\"No images found in {input_dir}. Supported: .jpg .jpeg .png\"\n",
    "real_batch = next(iter(dataloader))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "plot_image(ax, \"Real Image\", real_batch[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0S8t5BySulh3"
   },
   "outputs": [],
   "source": [
    "# Anti-alias blur (StyleGAN-ish upfirdn-lite)\n",
    "def _make_kernel(k):\n",
    "    k = torch.tensor(k, dtype=torch.float32)\n",
    "    if k.ndim == 1:\n",
    "        k = k[:, None] * k[None, :]\n",
    "    k /= k.sum()\n",
    "    return k\n",
    "\n",
    "class Blur(nn.Module):\n",
    "    def __init__(self, channels: int, kernel=(1,3,3,1), pad=(1,1)):\n",
    "        super().__init__()\n",
    "        k = _make_kernel(kernel)\n",
    "        w = k.view(1,1,k.shape[0],k.shape[1]).repeat(channels,1,1,1)\n",
    "        self.register_buffer(\"weight\", w)\n",
    "        self.pad = pad\n",
    "        self.channels = channels\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.weight, None, 1, self.pad, 1, self.channels)\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__(); self.eps = eps\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(torch.mean(x * x, dim=1, keepdim=True) + self.eps)\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1, ch, 1, 1))\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x  # deterministic at eval/export/ONNX\n",
    "        n = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n",
    "        return x + self.weight * n\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, use_blur=True, use_pn=False):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "        self.blur = Blur(in_ch, (1,3,3,1), (1,1)) if use_blur else nn.Identity()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        self.act1  = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.noise1 = NoiseInjection(out_ch)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
    "        self.act2  = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.noise2 = NoiseInjection(out_ch)\n",
    "\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1)  # light residual path\n",
    "        self.pn = PixelNorm() if use_pn else nn.Identity()\n",
    "\n",
    "        # init\n",
    "        for m in [self.conv1, self.conv2, self.skip]:\n",
    "            nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.up(x)\n",
    "        y = self.blur(y)\n",
    "        s = self.skip(y)\n",
    "\n",
    "        y = self.conv1(y); y = self.act1(y); y = self.noise1(y)\n",
    "        y = self.conv2(y); y = self.act2(y); y = self.pn(y)\n",
    "        return y + 0.1 * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFmjby3Tum_G"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    512x512 generator with two convs per scale, early blur, early pixelnorm,\n",
    "    light residual, and noise injection (train-only). ONNX-safe.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=128, img_channels=3, chs=(512, 512, 256, 128, 64, 32, 16, 8)):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.chs = chs\n",
    "        self.fc = nn.Linear(z_dim, 4 * 4 * chs[0])\n",
    "\n",
    "        blur_flags = [True, True, True, False, False, False, False]   # only early\n",
    "        pn_flags   = [True, True, True, False, False, False, False]\n",
    "\n",
    "        blocks = []\n",
    "        for i, (cin, cout) in enumerate(zip(chs[:-1], chs[1:])):\n",
    "            blocks.append(UpBlock(cin, cout, use_blur=blur_flags[i], use_pn=pn_flags[i]))\n",
    "        self.up = nn.Sequential(*blocks)\n",
    "        self.to_rgb = nn.Conv2d(chs[-1], img_channels, kernel_size=1)\n",
    "        nn.init.xavier_uniform_(self.to_rgb.weight); nn.init.zeros_(self.to_rgb.bias)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z.view(z.size(0), -1)).view(-1, self.chs[0], 4, 4)\n",
    "        x = self.up(x)\n",
    "        return torch.tanh(self.to_rgb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XoK7HKXuofx"
   },
   "outputs": [],
   "source": [
    "class MinibatchStdDev(nn.Module):\n",
    "    def __init__(self, group_size=4):\n",
    "        super().__init__()\n",
    "        self.group_size = group_size\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        g = min(self.group_size, N)  # handle small batches\n",
    "        y = x.view(g, -1, C, H, W)   # [g, n, C, H, W]\n",
    "        y = torch.var(y, dim=0, unbiased=False) + 1e-8\n",
    "        y = torch.sqrt(y)\n",
    "        y = torch.mean(y, dim=[1,2,3], keepdim=True)  # [1,1,1,1]\n",
    "        y = y.repeat(g, 1, H, W)                      # [N,1,H,W]\n",
    "        return torch.cat([x, y], dim=1)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3, chs=(64, 128, 256, 512, 512, 512, 512)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_ch = img_channels\n",
    "        for out_ch in chs:\n",
    "            layers += [\n",
    "                spectral_norm(nn.Conv2d(in_ch, out_ch, 4, 2, 1)),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]\n",
    "            in_ch = out_ch\n",
    "        self.body = nn.Sequential(*layers)\n",
    "        self.mbstd = MinibatchStdDev(group_size=4)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head = spectral_norm(nn.Linear(in_ch + 1, 1))  # +1 from mbstd\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.body(x)\n",
    "        h = self.mbstd(h)\n",
    "        h = self.pool(h).view(h.size(0), -1)\n",
    "        return self.head(h).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuhzmZFquqEg"
   },
   "outputs": [],
   "source": [
    "# ==== Training helpers: EMA, R1, DiffAugment ====\n",
    "\n",
    "def r1_penalty(d_out, real_img):\n",
    "    \"\"\"R1 gradient penalty on real images.\"\"\"\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_out.sum(), inputs=real_img,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    return grads.pow(2).reshape(grads.size(0), -1).sum(dim=1).mean()\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average of model weights.\"\"\"\n",
    "    def __init__(self, model, decay=0.995):\n",
    "        self.decay = decay\n",
    "        self.shadow = copy.deepcopy(model).eval()\n",
    "        for p in self.shadow.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        msd = self.shadow.state_dict()\n",
    "        for k, src in model.state_dict().items():\n",
    "            tgt = msd[k]\n",
    "            # Blend only floating-point tensors; copy integer buffers (e.g., counters)\n",
    "            if torch.is_floating_point(tgt):\n",
    "                tgt.mul_(self.decay).add_(src, alpha=1.0 - self.decay)\n",
    "            else:\n",
    "                tgt.copy_(src)\n",
    "\n",
    "# ---- DiffAugment (color, translation, cutout) ----\n",
    "def rand_brightness(x):  # ±0.1\n",
    "    return x + (torch.rand(x.size(0),1,1,1, device=x.device) - 0.5) * 0.2\n",
    "\n",
    "def rand_saturation(x):\n",
    "    xm = x.mean(dim=1, keepdim=True)\n",
    "    s = (torch.rand(x.size(0),1,1,1, device=x.device) * 0.6 + 0.7)  # 0.7..1.3\n",
    "    return (x - xm) * s + xm\n",
    "\n",
    "def rand_contrast(x):\n",
    "    xm = x.mean(dim=[1,2,3], keepdim=True)\n",
    "    c = (torch.rand(x.size(0),1,1,1, device=x.device) * 0.6 + 0.7)  # 0.7..1.3\n",
    "    return (x - xm) * c + xm\n",
    "\n",
    "def _translate_grid(x, ratio=0.06):\n",
    "    B,C,H,W = x.shape\n",
    "    shift = torch.randint(-int(W*ratio), int(W*ratio)+1, (B,2), device=x.device).float()\n",
    "    yy, xx = torch.meshgrid(\n",
    "        torch.linspace(-1,1,H,device=x.device),\n",
    "        torch.linspace(-1,1,W,device=x.device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    base = torch.stack((xx,yy), dim=-1).unsqueeze(0).repeat(B,1,1,1)\n",
    "    base[...,0] += shift[:,0].view(B,1,1) * (2.0/W)\n",
    "    base[...,1] += shift[:,1].view(B,1,1) * (2.0/H)\n",
    "    return F.grid_sample(x, base, padding_mode='reflection', align_corners=False)\n",
    "\n",
    "def _cutout(x, ratio=0.12):\n",
    "    B,C,H,W = x.shape\n",
    "    sz = max(1, int(H*ratio*0.5))\n",
    "    cx = torch.randint(sz, W - sz + 1, (B,), device=x.device)\n",
    "    cy = torch.randint(sz, H - sz + 1, (B,), device=x.device)\n",
    "    mask = torch.ones((B,1,H,W), device=x.device)\n",
    "    for i in range(B):\n",
    "        mask[i,:, cy[i]-sz:cy[i]+sz, cx[i]-sz:cx[i]+sz] = 0.0\n",
    "    return x * mask\n",
    "\n",
    "def diff_augment(x):\n",
    "    x = rand_brightness(x); x = rand_saturation(x); x = rand_contrast(x)\n",
    "    x = _translate_grid(x, ratio=0.125)\n",
    "    x = _cutout(x, ratio=0.25)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "968aaAGLurrj"
   },
   "outputs": [],
   "source": [
    "# Load snapshot if available\n",
    "def get_latest_snapshot(output_dir):\n",
    "    snapshots = glob.glob(os.path.join(output_dir, \"snapshot_epoch_*.pth\"))\n",
    "    if not snapshots:\n",
    "        return None\n",
    "    return max(snapshots, key=os.path.getctime)\n",
    "\n",
    "def get_latest_generator(output_dir):\n",
    "    generators = glob.glob(os.path.join(output_dir, \"generator_epoch_*.onnx\"))\n",
    "    if not generators:\n",
    "        return None\n",
    "    return max(generators, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqrPl_uVuuEc"
   },
   "outputs": [],
   "source": [
    "def load_snapshot(generator, discriminator, g_optimizer, d_optimizer, snapshot_path, ema_shadow=None):\n",
    "    checkpoint = torch.load(snapshot_path, map_location=device, weights_only=False)\n",
    "    generator.load_state_dict(checkpoint[\"generator\"])\n",
    "    discriminator.load_state_dict(checkpoint[\"discriminator\"])\n",
    "    g_optimizer.load_state_dict(checkpoint[\"g_optimizer\"])\n",
    "    d_optimizer.load_state_dict(checkpoint[\"d_optimizer\"])\n",
    "    if ema_shadow is not None and \"ema_generator\" in checkpoint:\n",
    "        ema_shadow.load_state_dict(checkpoint[\"ema_generator\"])\n",
    "    start_epoch = int(os.path.basename(snapshot_path).split(\"_\")[2].split(\".\")[0])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO3efseEuvlZ"
   },
   "outputs": [],
   "source": [
    "# Create the training loop (unconditional GAN, hinge + R1, SN-D, EMA, AMP)\n",
    "def train(generator, discriminator, dataloader, opts):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.train().to(device)\n",
    "    discriminator.train().to(device)\n",
    "\n",
    "    g_opt = optim.Adam(generator.parameters(), lr=opts.g_lr, betas=opts.betas)\n",
    "    d_opt = optim.Adam(discriminator.parameters(), lr=opts.d_lr, betas=opts.betas)\n",
    "\n",
    "    scaler_g = GradScaler(enabled=opts.use_amp)\n",
    "    scaler_d = GradScaler(enabled=opts.use_amp)\n",
    "\n",
    "    ema = EMA(generator, decay=opts.ema_decay)\n",
    "\n",
    "    # Fixed latent for monitoring\n",
    "    fixed_z = torch.randn(16, opts.z_dim, device=device)\n",
    "\n",
    "    start_epoch = 1\n",
    "    if not getattr(opts, \"restart\", False):\n",
    "        latest_snapshot = get_latest_snapshot(opts.output_dir)\n",
    "        if latest_snapshot:\n",
    "            start_epoch = load_snapshot(\n",
    "                generator, discriminator, g_opt, d_opt, latest_snapshot, ema_shadow=ema.shadow\n",
    "            )\n",
    "            print(f\"Resumed from {latest_snapshot} (start_epoch={start_epoch})\")\n",
    "\n",
    "    it = 0\n",
    "    for epoch in range(start_epoch, opts.epochs + 1):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{opts.epochs}\")\n",
    "        for real in pbar:\n",
    "            it += 1\n",
    "            real = real.to(device, non_blocking=True)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # ---- Train Discriminator ----\n",
    "            z = torch.randn(bsz, opts.z_dim, device=device)\n",
    "            with autocast(enabled=opts.use_amp):\n",
    "                with torch.no_grad():\n",
    "                    fake = generator(z)\n",
    "                # Apply DiffAugment on both\n",
    "                real_aug = diff_augment(real)\n",
    "                fake_aug = diff_augment(fake)\n",
    "                d_real = discriminator(real_aug)\n",
    "                d_fake = discriminator(fake_aug)\n",
    "                d_loss = F.relu(1.0 - d_real).mean() + F.relu(1.0 + d_fake).mean()\n",
    "            \n",
    "            d_opt.zero_grad(set_to_none=True)\n",
    "            scaler_d.scale(d_loss).backward()\n",
    "            scaler_d.step(d_opt)\n",
    "            scaler_d.update()\n",
    "\n",
    "            # ---- Lazy R1 on augmented real (in FP32 for stability) ----\n",
    "            if (it % opts.d_reg_every) == 0:\n",
    "                real_req = diff_augment(real.detach().requires_grad_(True))\n",
    "                with autocast(enabled=False):\n",
    "                    d_real_r1 = discriminator(real_req.float())\n",
    "                    r1 = r1_penalty(d_real_r1, real_req.float())\n",
    "                    r1_loss = (opts.r1_gamma / 2.0) * r1\n",
    "                d_opt.zero_grad(set_to_none=True)\n",
    "                scaler_d.scale(r1_loss).backward()\n",
    "                scaler_d.step(d_opt)\n",
    "                scaler_d.update()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Generator\n",
    "            # -----------------------\n",
    "            z = torch.randn(bsz, opts.z_dim, device=device)\n",
    "            with autocast(enabled=opts.use_amp):\n",
    "                fake = generator(z)\n",
    "                g_fake = discriminator(fake)\n",
    "                g_loss = -g_fake.mean()\n",
    "\n",
    "            g_opt.zero_grad(set_to_none=True)\n",
    "            scaler_g.scale(g_loss).backward()\n",
    "            scaler_g.step(g_opt)\n",
    "            scaler_g.update()\n",
    "\n",
    "            # EMA update\n",
    "            ema.update(generator)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"D\": f\"{d_loss.item():.3f}\",\n",
    "                \"G\": f\"{g_loss.item():.3f}\",\n",
    "            })\n",
    "\n",
    "        # Visualization and sampling (always raw G)\n",
    "        if it % opts.sample_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                generator.eval() # Avoid noise injection during visualization\n",
    "                samples = generator(fixed_z).detach().cpu()\n",
    "                # Save and show\n",
    "                save_path = os.path.join(opts.output_dir, f\"epoch_{epoch}_iter_{it}.jpg\")\n",
    "                save_image(samples, save_path, nrow=4, normalize=True, value_range=(-1, 1))\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Epoch {epoch} | iter {it}\")\n",
    "                grid = (samples[:4] + 1) / 2\n",
    "                fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
    "                for a, img in zip(axes, grid):\n",
    "                    a.imshow(img.permute(1, 2, 0).numpy()); a.axis(\"off\")\n",
    "                plt.show()\n",
    "                generator.train()\n",
    "\n",
    "        # Snapshot & ONNX export at epoch end\n",
    "        if (epoch % opts.snapshot_interval) == 0:\n",
    "            snap_path = os.path.join(opts.output_dir, f\"snapshot_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                \"generator\": generator.state_dict(),\n",
    "                \"discriminator\": discriminator.state_dict(),\n",
    "                \"g_optimizer\": g_opt.state_dict(),\n",
    "                \"d_optimizer\": d_opt.state_dict(),\n",
    "                \"ema_generator\": ema.shadow.state_dict(),\n",
    "            }, snap_path)\n",
    "            print(f\"Saved snapshot to {snap_path}\")\n",
    "\n",
    "            # Export ONNX from RAW generator\n",
    "            generator.eval()\n",
    "            dummy_z = torch.randn(1, opts.z_dim, device=device)\n",
    "            onnx_path = os.path.join(opts.output_dir, f\"generator_epoch_{epoch}.onnx\")\n",
    "            torch.onnx.export(\n",
    "                generator,\n",
    "                (dummy_z,),\n",
    "                onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=17,\n",
    "                do_constant_folding=True,\n",
    "                input_names=[\"z\"],\n",
    "                output_names=[\"image\"],\n",
    "                dynamic_axes={\"z\": {0: \"batch\"}, \"image\": {0: \"batch\"}},\n",
    "            )\n",
    "            print(f\"ONNX model exported to {onnx_path}\")\n",
    "            generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "2QUHdooiuztD",
    "outputId": "a26f1c1a-794b-4cfc-9ef8-0dff8dbd83a0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(z_dim=z_dim, img_channels=img_channels).to(device)\n",
    "discriminator = Discriminator(img_channels=img_channels).to(device)\n",
    "\n",
    "opts = SimpleNamespace(\n",
    "    output_dir=output_dir,\n",
    "    sample_interval=sample_interval,\n",
    "    snapshot_interval=snapshot_interval,\n",
    "    epochs=epochs,\n",
    "    restart=False,\n",
    "    # new:\n",
    "    z_dim=z_dim,\n",
    "    g_lr=g_lr, d_lr=d_lr, betas=betas,\n",
    "    d_reg_every=d_reg_every, r1_gamma=r1_gamma,\n",
    "    ema_decay=ema_decay, use_amp=use_amp,\n",
    ")\n",
    "\n",
    "train(generator, discriminator, dataloader, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
