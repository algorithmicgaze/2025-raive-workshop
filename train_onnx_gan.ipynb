{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_mp0c0k8rCS"
   },
   "source": [
    "# Training a PIX2PIX Model using PyTorch / ONNX\n",
    "\n",
    "This notebook walks you through the steps of training your own image-to-image machine learning model.\n",
    "\n",
    "Basically all you have to do is put your cursor in a cell and press Shift+Enter. At the end, you can download the latest model from the `output` folder (it will be called something like `generator_epoch_XXX.onnx`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqhpNeaL8z7C",
    "outputId": "bd15bb05-4640-4dd2-c43c-08fd8f0c70e7"
   },
   "outputs": [],
   "source": [
    "# Make sure you are connected to a runtime with a GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFa-5ik3_MFq",
    "outputId": "f5eeabe8-246d-49f4-cced-1ad32e0b5b02"
   },
   "outputs": [],
   "source": [
    "# Install ONNX (not installed by default)\n",
    "#import locale\n",
    "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "%pip install -q onnx matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcftxzAmuAzP"
   },
   "outputs": [],
   "source": [
    "# Import all other dependencies\n",
    "import glob, os, random, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJijQKrV9J93",
    "outputId": "fb4fc26a-ce98-4baf-c5c9-397f5ce09d2f"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(\"GPU is\", \"available\" if gpu_available else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the dataset\n",
    "!curl -O https://algorithmicgaze.s3.amazonaws.com/workshops/2025-raive/patterns_512.zip\n",
    "!mkdir -p datasets/patterns\n",
    "!unzip -j -o -qq *.zip -d datasets/patterns\n",
    "!rm -r datasets/patterns/._*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-1v5IkJ-Nex"
   },
   "outputs": [],
   "source": [
    "# Some helper functions for creating/checking directories.\n",
    "def directory_should_exist(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    if not os.path.isdir(dir):\n",
    "        raise Exception(\"Path '{}' is not a directory.\".format(dir))\n",
    "    return dir\n",
    "\n",
    "def ensure_directory(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2O3py7u-JC"
   },
   "outputs": [],
   "source": [
    "# Point to your dataset and configure training\n",
    "input_dir = directory_should_exist(\"datasets/patterns\")\n",
    "output_dir = ensure_directory(\"output\")\n",
    "\n",
    "# I/O and schedule\n",
    "epochs = 100\n",
    "batch_size = 64               # 512^2 is heavy; 4–8 is typical\n",
    "sample_interval = 15          # iters between previews\n",
    "snapshot_interval = 1        # epochs between checkpoints\n",
    "\n",
    "# Latent & image\n",
    "z_dim = 128\n",
    "img_channels = 3\n",
    "image_size = 512\n",
    "\n",
    "# Optimizer (classic DCGAN)\n",
    "g_lr = 2e-4\n",
    "d_lr = 2e-4\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# Label smoothing\n",
    "real_label_smooth = 0.1      # real target = 1 - 0.1 = 0.9\n",
    "\n",
    "# AMP\n",
    "use_amp = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJuVKp5GujGO"
   },
   "outputs": [],
   "source": [
    "# Unconditional image dataset (no splitting) + shape guard\n",
    "class UncondImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, expected_hw=None):\n",
    "        \"\"\"\n",
    "        expected_hw: tuple (H, W) to enforce final tensor size, e.g. (512, 512).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.expected_hw = expected_hw\n",
    "        self.image_files = [f for f in os.listdir(root_dir)\n",
    "                            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # [C,H,W], float in [-1,1]\n",
    "        if self.expected_hw is not None:\n",
    "            C, H, W = img.shape\n",
    "            expH, expW = self.expected_hw\n",
    "            if (H, W) != (expH, expW) or C != 3:\n",
    "                raise ValueError(f\"Bad tensor shape {img.shape} for {path}; expected (3,{expH},{expW})\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTRhTxTUNd-d"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # -> [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = UncondImageDataset(input_dir, transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "z1CkVmZINZo8",
    "outputId": "5856e7ec-827d-499a-bcfa-5c0e2f16c880"
   },
   "outputs": [],
   "source": [
    "# Show a single real image from the dataset\n",
    "def plot_image(ax, title, img):\n",
    "    img = (img + 1) / 2\n",
    "    ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    ax.set_title(title); ax.axis(\"off\")\n",
    "\n",
    "assert len(dataset) > 0, f\"No images found in {input_dir}. Supported: .jpg .jpeg .png\"\n",
    "real_batch = next(iter(dataloader))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "plot_image(ax, \"Real Image\", real_batch[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN weight init (as in the paper)\n",
    "def weights_init_dcgan(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('ConvTranspose') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if getattr(m, 'bias', None) is not None:\n",
    "            nn.init.zeros_(m.bias.data)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFmjby3Tum_G"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN generator for 512x512.\n",
    "    z -> [B, z, 1,1] -> deconvs doubling size each step: 4→8→…→512\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=128, img_channels=3, base=64):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.main = nn.Sequential(\n",
    "            # 1x1 -> 4x4\n",
    "            nn.ConvTranspose2d(z_dim, base*16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(base*16), nn.ReLU(True),\n",
    "\n",
    "            # 4 -> 8\n",
    "            nn.ConvTranspose2d(base*16, base*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*8), nn.ReLU(True),\n",
    "\n",
    "            # 8 -> 16\n",
    "            nn.ConvTranspose2d(base*8, base*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*4), nn.ReLU(True),\n",
    "\n",
    "            # 16 -> 32\n",
    "            nn.ConvTranspose2d(base*4, base*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*2), nn.ReLU(True),\n",
    "\n",
    "            # 32 -> 64\n",
    "            nn.ConvTranspose2d(base*2, base, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base), nn.ReLU(True),\n",
    "\n",
    "            # 64 -> 128\n",
    "            nn.ConvTranspose2d(base, base//2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base//2), nn.ReLU(True),\n",
    "\n",
    "            # 128 -> 256\n",
    "            nn.ConvTranspose2d(base//2, base//4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base//4), nn.ReLU(True),\n",
    "\n",
    "            # 256 -> 512\n",
    "            nn.ConvTranspose2d(base//4, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.apply(weights_init_dcgan)\n",
    "\n",
    "    def forward(self, z):\n",
    "        if z.dim() == 2:\n",
    "            z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "        return self.main(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XoK7HKXuofx"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN discriminator for 512x512.\n",
    "    Strided 4x4 convs halve size each step: 512→…→4, then 1x1 logit.\n",
    "    BatchNorm everywhere except the first block (classic DCGAN).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_channels=3, base=64):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 512 -> 256\n",
    "            nn.Conv2d(img_channels, base, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 256 -> 128\n",
    "            nn.Conv2d(base, base*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 128 -> 64\n",
    "            nn.Conv2d(base*2, base*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 64 -> 32\n",
    "            nn.Conv2d(base*4, base*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 32 -> 16\n",
    "            nn.Conv2d(base*8, base*16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 16 -> 8\n",
    "            nn.Conv2d(base*16, base*16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 8 -> 4\n",
    "            nn.Conv2d(base*16, base*16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 4 -> 1x1\n",
    "            nn.Conv2d(base*16, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "        self.apply(weights_init_dcgan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)       # [B, 1, 1, 1]\n",
    "        return out.view(-1)      # logits (no sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuhzmZFquqEg"
   },
   "outputs": [],
   "source": [
    "# Non-saturating BCE losses (classic DCGAN)\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def real_targets(b, device):\n",
    "    # one-sided label smoothing\n",
    "    return torch.full((b,), 1.0 - real_label_smooth, device=device)\n",
    "\n",
    "def fake_targets(b, device):\n",
    "    return torch.zeros(b, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "968aaAGLurrj"
   },
   "outputs": [],
   "source": [
    "# Load snapshot if available\n",
    "def get_latest_snapshot(output_dir):\n",
    "    snapshots = glob.glob(os.path.join(output_dir, \"snapshot_epoch_*.pth\"))\n",
    "    if not snapshots:\n",
    "        return None\n",
    "    return max(snapshots, key=os.path.getctime)\n",
    "\n",
    "def get_latest_generator(output_dir):\n",
    "    generators = glob.glob(os.path.join(output_dir, \"generator_epoch_*.onnx\"))\n",
    "    if not generators:\n",
    "        return None\n",
    "    return max(generators, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqrPl_uVuuEc"
   },
   "outputs": [],
   "source": [
    "def load_snapshot(generator, discriminator, g_optimizer, d_optimizer, snapshot_path, ema_shadow=None):\n",
    "    checkpoint = torch.load(snapshot_path, map_location=device, weights_only=False)\n",
    "    generator.load_state_dict(checkpoint[\"generator\"])\n",
    "    discriminator.load_state_dict(checkpoint[\"discriminator\"])\n",
    "    g_optimizer.load_state_dict(checkpoint[\"g_optimizer\"])\n",
    "    d_optimizer.load_state_dict(checkpoint[\"d_optimizer\"])\n",
    "    if ema_shadow is not None and \"ema_generator\" in checkpoint:\n",
    "        ema_shadow.load_state_dict(checkpoint[\"ema_generator\"])\n",
    "    start_epoch = int(os.path.basename(snapshot_path).split(\"_\")[2].split(\".\")[0])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO3efseEuvlZ"
   },
   "outputs": [],
   "source": [
    "def train(generator, discriminator, dataloader, opts):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.train().to(device)\n",
    "    discriminator.train().to(device)\n",
    "\n",
    "    g_opt = optim.Adam(generator.parameters(), lr=opts.g_lr, betas=opts.betas)\n",
    "    d_opt = optim.Adam(discriminator.parameters(), lr=opts.d_lr, betas=opts.betas)\n",
    "    scaler_g = GradScaler(enabled=opts.use_amp)\n",
    "    scaler_d = GradScaler(enabled=opts.use_amp)\n",
    "\n",
    "    fixed_z = torch.randn(16, opts.z_dim, device=device)\n",
    "    it = 0\n",
    "    for epoch in range(1, opts.epochs + 1):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{opts.epochs}\")\n",
    "        for real in pbar:\n",
    "            it += 1\n",
    "            real = real.to(device, non_blocking=True)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # ------------------ Train D ------------------\n",
    "            z = torch.randn(bsz, opts.z_dim, device=device)\n",
    "            with autocast(enabled=opts.use_amp):\n",
    "                with torch.no_grad():\n",
    "                    fake = generator(z)\n",
    "\n",
    "                d_real = discriminator(real)\n",
    "                d_fake = discriminator(fake)\n",
    "\n",
    "                d_loss_real = bce(d_real, real_targets(bsz, device))\n",
    "                d_loss_fake = bce(d_fake, fake_targets(bsz, device))\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            d_opt.zero_grad(set_to_none=True)\n",
    "            scaler_d.scale(d_loss).backward()\n",
    "            scaler_d.step(d_opt)\n",
    "            scaler_d.update()\n",
    "\n",
    "            # ------------------ Train G ------------------\n",
    "            z = torch.randn(bsz, opts.z_dim, device=device)\n",
    "            with autocast(enabled=opts.use_amp):\n",
    "                fake = generator(z)\n",
    "                g_fake = discriminator(fake)\n",
    "                # Non-saturating: tell D \"these are real\"\n",
    "                g_loss = bce(g_fake, real_targets(bsz, device))\n",
    "\n",
    "            g_opt.zero_grad(set_to_none=True)\n",
    "            scaler_g.scale(g_loss).backward()\n",
    "            scaler_g.step(g_opt)\n",
    "            scaler_g.update()\n",
    "\n",
    "            pbar.set_postfix({\"D\": f\"{d_loss.item():.3f}\", \"G\": f\"{g_loss.item():.3f}\"})\n",
    "\n",
    "            # --------- Preview (always raw G) ----------\n",
    "            if it % opts.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    generator.eval()\n",
    "                    samples = generator(fixed_z).detach().cpu()\n",
    "                    save_path = os.path.join(opts.output_dir, f\"epoch_{epoch}_iter_{it}.jpg\")\n",
    "                    save_image(samples, save_path, nrow=4, normalize=True, value_range=(-1, 1))\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"Epoch {epoch} | iter {it}\")\n",
    "                    grid = (samples[:4] + 1) / 2\n",
    "                    fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
    "                    for a, img in zip(axes, grid):\n",
    "                        a.imshow(img.permute(1, 2, 0).numpy()); a.axis(\"off\")\n",
    "                    plt.show()\n",
    "                    generator.train()\n",
    "\n",
    "        # --------- Snapshot + ONNX (raw G) ----------\n",
    "        if (epoch % opts.snapshot_interval) == 0:\n",
    "            snap_path = os.path.join(opts.output_dir, f\"snapshot_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                \"generator\": generator.state_dict(),\n",
    "                \"discriminator\": discriminator.state_dict(),\n",
    "                \"g_optimizer\": g_opt.state_dict(),\n",
    "                \"d_optimizer\": d_opt.state_dict(),\n",
    "            }, snap_path)\n",
    "            print(f\"Saved snapshot to {snap_path}\")\n",
    "\n",
    "            generator.eval()\n",
    "            dummy_z = torch.randn(1, opts.z_dim, device=device)\n",
    "            onnx_path = os.path.join(opts.output_dir, f\"generator_epoch_{epoch}.onnx\")\n",
    "            torch.onnx.export(\n",
    "                generator,\n",
    "                (dummy_z,),\n",
    "                onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=17,\n",
    "                do_constant_folding=True,\n",
    "                input_names=[\"z\"],\n",
    "                output_names=[\"image\"],\n",
    "                dynamic_axes={\"z\": {0: \"batch\"}, \"image\": {0: \"batch\"}},\n",
    "            )\n",
    "            print(f\"ONNX model exported to {onnx_path}\")\n",
    "            generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "2QUHdooiuztD",
    "outputId": "a26f1c1a-794b-4cfc-9ef8-0dff8dbd83a0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(z_dim=z_dim, img_channels=img_channels).to(device)\n",
    "discriminator = Discriminator(img_channels=img_channels).to(device)\n",
    "\n",
    "opts = SimpleNamespace(\n",
    "    output_dir=output_dir,\n",
    "    sample_interval=sample_interval,\n",
    "    snapshot_interval=snapshot_interval,\n",
    "    epochs=epochs,\n",
    "    z_dim=z_dim,\n",
    "    g_lr=g_lr, d_lr=d_lr, betas=betas,\n",
    "    use_amp=use_amp,\n",
    ")\n",
    "\n",
    "# Fresh run recommended if you changed architectures\n",
    "train(generator, discriminator, dataloader, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
